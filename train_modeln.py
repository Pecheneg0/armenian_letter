# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR  # –î–æ–±–∞–≤–ª–µ–Ω Learning Rate Scheduler
from modeln import ArmenianLetterNet

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize((32, 32)),
    transforms.RandomRotation(30),  # –ü–æ–≤–æ—Ä–æ—Ç—ã
    transforms.ColorJitter(brightness=0.3, contrast=0.3),  # –Ø—Ä–∫–æ—Å—Ç—å/–∫–æ–Ω—Ç—Ä–∞—Å—Ç
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # –°–º–µ—â–µ–Ω–∏–µ
    transforms.RandomHorizontalFlip(),  # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
])

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
train_dataset = datasets.ImageFolder(root="/Users/aleksandr/Downloads/–°–∫–∞—Ç/dataset_generated_2", transform=transform)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # –£–≤–µ–ª–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
model = ArmenianLetterNet()
criterion = nn.CrossEntropyLoss()  # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # –î–æ–±–∞–≤–ª–µ–Ω–∞ L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Learning Rate Scheduler

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
num_epochs = 30  # –£–≤–µ–ª–∏—á–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
for epoch in range(num_epochs):
    model.train()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        optimizer.zero_grad()  # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        outputs = model(images)
        loss = criterion(outputs, labels)  # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä—é
        loss.backward()  # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
        optimizer.step()  # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
    scheduler.step()

    accuracy = 100 * correct / total
    print(f"üìä –≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs} | –ü–æ—Ç–µ—Ä—è: {running_loss:.4f} | –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2f}%")

print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
torch.save(model.state_dict(), "armenian_letters_model_x_TF.pth")
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")